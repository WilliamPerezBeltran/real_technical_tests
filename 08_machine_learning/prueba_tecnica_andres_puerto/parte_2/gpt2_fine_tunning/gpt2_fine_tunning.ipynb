{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Instalaci√≥n de paquetes"
      ],
      "metadata": {
        "id": "JZFmFbYPye10"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2J7e1EGIkzI",
        "outputId": "814484be-4ba3-4fdf-bc8d-fa8685d340ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.32.1)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.5)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Downloading accelerate-0.33.0-py3-none-any.whl (315 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m315.1/315.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 0.32.1\n",
            "    Uninstalling accelerate-0.32.1:\n",
            "      Successfully uninstalled accelerate-0.32.1\n",
            "Successfully installed accelerate-0.33.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.4)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.33.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.3.1+cu121)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.5.82)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# Pytorch viene por defecto en google colab\n",
        "!pip install accelerate -U\n",
        "!pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "wb67A2g6Itr4"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# funci√≥n para calcular perplexidad"
      ],
      "metadata": {
        "id": "wEwuNkEMyjjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_perplexity(text,tokenizer,model,device):\n",
        "  model=model.to(device)\n",
        "  inputs = tokenizer(text, return_tensors = \"pt\").to(device)\n",
        "  loss = model(input_ids = inputs[\"input_ids\"].to(device), labels = inputs[\"input_ids\"].to(device)).loss\n",
        "  ppl = torch.exp(loss)\n",
        "  return ppl.item()\n"
      ],
      "metadata": {
        "id": "i9qm7OTbbxMa"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = \"cuda:0\"\n",
        "else:\n",
        "    device = \"cpu\""
      ],
      "metadata": {
        "id": "jzMNeKTwZpDO"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo Base"
      ],
      "metadata": {
        "id": "sKWJh-YcyqlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_id='gpt2'\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "\n",
        ")\n",
        "\n",
        "base_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        ")"
      ],
      "metadata": {
        "id": "L9_e_NFIcI3F"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Se genera Texto acerca de Duitama y se calcula su respectiva perplexidad"
      ],
      "metadata": {
        "id": "OxKxa_C2yuoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_inputs = base_tokenizer(\"Duitama is\", return_tensors = \"pt\").to(device)\n"
      ],
      "metadata": {
        "id": "S9oZG6cHqq2I"
      },
      "execution_count": 334,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_tokenizer.decode(base_model.generate(base_inputs['input_ids'],max_new_tokens=200)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "qgQOVFZyq1Bg",
        "outputId": "5a79b380-ddb2-48e8-b239-d18d682e1458"
      },
      "execution_count": 335,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Duitama is a small, but very well-built, and very well-designed, and very well-designed. It's a very good design, and it's very well-designed. It's a very good design. It's a very good design. It's a very good design. It's a very good design. It's a very good design. It's a very good design. It's a very good design. It's a very good design. It's a very good design. It's a very good design. It's a very good design. It's a very good design. It's a very good design. It's a very good design. It's a very good design. It's a very good design. It's a very good design. It's a very good design. It's a very good design. It's a very good design. It's a very good design. It's a very good design. It's a very good design. It's a\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 335
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_perplexity(\"Duitama is\",base_tokenizer,base_model,device) # initial perplexity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwtRYCL9cjoj",
        "outputId": "449d68eb-36ac-4643-cf34-d3ac6c65fc99"
      },
      "execution_count": 336,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1399.706787109375"
            ]
          },
          "metadata": {},
          "execution_count": 336
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_inputs = base_tokenizer(\"Duitama is important\", return_tensors = \"pt\").to(device)"
      ],
      "metadata": {
        "id": "mhbh_5ZluxuB"
      },
      "execution_count": 330,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_tokenizer.decode(base_model.generate(base_inputs['input_ids'],max_new_tokens=200)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "5X5L_reuu2b7",
        "outputId": "cef95dfe-9d58-4571-acd9-2805cc25e4fd"
      },
      "execution_count": 332,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Duitama is important to me because it's a place where I can go to learn about the world and the people I love. It's a place where I can go to learn about the world and the people I love. It's a place where I can go to learn about the world and the people I love.\\n\\nI'm a big fan of the show, and I'm really excited to see what the next season will bring. I'm a big fan of the show, and I'm really excited to see what the next season will bring. I'm a big fan of the show, and I'm really excited to see what the next season will bring.\\n\\nI'm a big fan of the show, and I'm really excited to see what the next season will bring. I'm a big fan of the show, and I'm really excited to see what the next season will bring.\\n\\nI'm a big fan of the show, and I'm really excited to see what the next\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 332
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_perplexity(\"Duitama is important\",base_tokenizer,base_model,device) # initial perplexity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjWIbp_Gu-NM",
        "outputId": "b5e13722-2915-4859-cab8-dae5748ff216"
      },
      "execution_count": 333,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1598.3819580078125"
            ]
          },
          "metadata": {},
          "execution_count": 333
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo para entrenar"
      ],
      "metadata": {
        "id": "jDXoVOgUy4WT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_id='gpt2'\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        ")"
      ],
      "metadata": {
        "id": "ZXzwT_I1JlsX"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#grouped_data['line_text']=grouped_data['line_text']+tokenizer.eos_token"
      ],
      "metadata": {
        "id": "1FG21IGaj9rY"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "fT1nWAlKJzaI"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Usando un archivo de texto sobre Duitama extraido de Wikipedia se transformar√° en dataset"
      ],
      "metadata": {
        "id": "Iw3JgXePy7Vy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=transformers.TextDataset(tokenizer=tokenizer,file_path='duitama.es.en.txt',block_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "im3U7rPCJ_se",
        "outputId": "fcd9aee8-817a-4254-9978-3dfb647057dc"
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (8837 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Se definen argumentos de entrenamiento\n",
        "\n"
      ],
      "metadata": {
        "id": "KLGqqXiizCIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = transformers.TrainingArguments(\n",
        "    output_dir=\"/content/output3\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=100,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    eval_steps = 400,\n",
        "    save_steps=800,\n",
        "    warmup_steps=100,\n",
        "    )"
      ],
      "metadata": {
        "id": "CpaUt37XKGpV"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = transformers.DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False,\n",
        "    )\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_data,\n",
        ")\n"
      ],
      "metadata": {
        "id": "iIIR0qA7MIIf"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento ( se dejaron pocas epocas por prop√≥sitos de practicidad en la prueba y porque google colab es limitado en el uso de gpu)"
      ],
      "metadata": {
        "id": "3KSz8OQ7zGc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "FuUj0N1rKbz2",
        "outputId": "1f3117ed-1ea2-4dbc-85b8-2a6e24f1de9d"
      },
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 04:13, Epoch 100/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.072400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=500, training_loss=1.072354248046875, metrics={'train_runtime': 254.3246, 'train_samples_per_second': 54.261, 'train_steps_per_second': 1.966, 'total_flos': 450728755200000.0, 'train_loss': 1.072354248046875, 'epoch': 100.0})"
            ]
          },
          "metadata": {},
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# se establece el pipeline para al inferencia"
      ],
      "metadata": {
        "id": "2WL1_sFizO5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bot=pipeline('text-generation',model='/content/output3/checkpoint-500',tokenizer=model_id,device=0,max_length=200,truncation=True)"
      ],
      "metadata": {
        "id": "MsZOh8nAOfqi"
      },
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/final_model.zip /content/output3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4H-nBHtzrAj",
        "outputId": "cb34f098-dd75-42c0-ab17-1e90a2d7fb9c"
      },
      "execution_count": 318,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/output3/ (stored 0%)\n",
            "  adding: content/output3/runs/ (stored 0%)\n",
            "  adding: content/output3/runs/Aug01_02-04-26_2f7ed00d4317/ (stored 0%)\n",
            "  adding: content/output3/runs/Aug01_02-04-26_2f7ed00d4317/events.out.tfevents.1722477873.2f7ed00d4317.858.4 (deflated 60%)\n",
            "  adding: content/output3/checkpoint-500/ (stored 0%)\n",
            "  adding: content/output3/checkpoint-500/trainer_state.json (deflated 56%)\n",
            "  adding: content/output3/checkpoint-500/optimizer.pt (deflated 8%)\n",
            "  adding: content/output3/checkpoint-500/config.json (deflated 51%)\n",
            "  adding: content/output3/checkpoint-500/model.safetensors (deflated 7%)\n",
            "  adding: content/output3/checkpoint-500/rng_state.pth (deflated 25%)\n",
            "  adding: content/output3/checkpoint-500/training_args.bin (deflated 51%)\n",
            "  adding: content/output3/checkpoint-500/scheduler.pt (deflated 56%)\n",
            "  adding: content/output3/checkpoint-500/generation_config.json (deflated 24%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Se hacen los mismo prompts que el modelo base y se calculan las perplexidades"
      ],
      "metadata": {
        "id": "tVqwsaINzTe8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"Duitama is\"\n",
        "final_output=bot(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxFgW_76OWYx",
        "outputId": "e050ebe7-8862-489e-bd5a-5b8e95d97d34"
      },
      "execution_count": 315,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_output[0]['generated_text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "23nZ0ouIOozg",
        "outputId": "9654bc4b-5f46-4559-ab60-3e3f2ecfcef3"
      },
      "execution_count": 316,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Duitama is a Colombian municipality, located in the department of Boyac√°, in central-eastern Colombia, in the Alto Chicamocha region. It is the capital and largest urban center in the province of Tundama. It is known as \"The Civic Capital of Boyac√°\" and \"The Civic Capital of Boyac√°\" and \"The Civic Capital of Boyac√°\" for its excellent industrial activity, commercial, transportation and cultural activities. The city is known as the \"Holland Highway\" and \"The Civic Capital of Boyac√°\" for its excellent access to the water resources of the rivers and streams that bathe the local area. The city is famous for its fruit orchards of apples, peaches, pears, curubas and plums; also grapes, wheat, barley and wines. The fruit orchards of apples, peaches, curubas and plums are known as the \"Pears of Boyac√°'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 316
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_perplexity(text,tokenizer,model,device)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c13oRmLzO-4J",
        "outputId": "cf918ae9-906a-4c4c-a840-86db351cb29a"
      },
      "execution_count": 317,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0838747024536133"
            ]
          },
          "metadata": {},
          "execution_count": 317
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"Duitama is important\"\n",
        "final_output=bot(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7Nk0jkVulPf",
        "outputId": "8a7d3ab7-6ba2-4d52-cf5e-fa7a2d0af5f8"
      },
      "execution_count": 306,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_output[0]['generated_text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "QyN3UVqbuo6v",
        "outputId": "f8cd642c-afd5-4048-aa62-9157c239e9cd"
      },
      "execution_count": 307,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Duitama is important for the transportation of cargo and it is the objective of the companies that provide it to carry out the maintenance of the structures that constitute the Contracting Post of the Municipality of Duitama. It is a strategic point for the provision of services and it is known as the 'Bundesliga of Cargo' because of its completeness and completeness as well as the fact that it is the first cargo transportation center in the country. Furthermore, it presents a productive industrial area and, above all, it presents a productive agricultural area. The agricultural area of Duitama is classified as one of the best in the world by the World Agricultural Organization, awarded the best possible status in the category of Agricultural Areas and, due to its large variety of crops and vegetables, it is one of the main food providers for the local population. The biological resources of the rivers and streams that bathe the landscape are recognized as being of exceptional quality and, owing to their high quality,\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 307
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_perplexity(text,tokenizer,model,device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pj10-K-eutEF",
        "outputId": "ac952b7f-c306-47d2-ca37-feae9edaec0e"
      },
      "execution_count": 308,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "70.11921691894531"
            ]
          },
          "metadata": {},
          "execution_count": 308
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y4u79K2pZ8rZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}